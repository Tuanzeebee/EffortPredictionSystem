# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
import streamlit as st
import pandas as pd
import numpy as np
import joblib  # ƒê·ªÉ t·∫£i m√¥ h√¨nh v√† preprocessors
import matplotlib.pyplot as plt  # Th√™m th∆∞ vi·ªán matplotlib
import os  # Th√™m ƒë·ªÉ l√†m vi·ªác v·ªõi ƒë∆∞·ªùng d·∫´n file
from collections import OrderedDict  # Th√™m ƒë·ªÉ gi·ªØ th·ª© t·ª± models
import math  # C·∫ßn cho COCOMO
import traceback  # Th√™m ƒë·ªÉ in l·ªói chi ti·∫øt

# Import c√°c l·ªõp c·∫ßn thi·∫øt t·ª´ scikit-learn (n·∫øu c·∫ßn tham chi·∫øu ki·ªÉu)
try:
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline
    from sklearn.impute import SimpleImputer
    from sklearn.preprocessing import RobustScaler, OneHotEncoder  # Ho·∫∑c StandardScaler n·∫øu b·∫°n d√πng
    from sklearn.linear_model import LinearRegression
    from sklearn.tree import DecisionTreeRegressor
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.neural_network import MLPRegressor
    import xgboost as xgb
except ImportError as e:
    st.error(f"L·ªói Import th∆∞ vi·ªán scikit-learn ho·∫∑c xgboost: {e}. H√£y ƒë·∫£m b·∫£o ch√∫ng ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t.")
    st.stop()

# --- C·∫•u h√¨nh Trang ---
st.set_page_config(page_title="So s√°nh ∆Ø·ªõc t√≠nh Effort Ph·∫ßn m·ªÅm", layout="wide")
st.title("·ª®ng d·ª•ng So s√°nh ∆Ø·ªõc t√≠nh Effort Ph·∫ßn m·ªÅm üìä")
st.write("""
Nh·∫≠p th√¥ng tin d·ª± √°n ƒë·ªÉ nh·∫≠n ∆∞·ªõc t√≠nh effort (person-hours) t·ª´ nhi·ªÅu m√¥ h√¨nh Machine Learning
v√† c√°c ph∆∞∆°ng ph√°p truy·ªÅn th·ªëng (COCOMO II Basic, Function Points, Use Case Points).
""")

# --- ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n v√† H·∫±ng s·ªë ---
# QUAN TR·ªåNG: ƒê·∫£m b·∫£o c√°c file .joblib (preprocessor.joblib, feature_names.joblib, v√† c√°c model .joblib)
# n·∫±m trong th∆∞ m·ª•c ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh b·ªüi OUTPUT_DIR.
# M·∫∑c ƒë·ªãnh l√† th∆∞ m·ª•c hi·ªán t·∫°i n∆°i ch·∫°y app.py.
OUTPUT_DIR = "."
PREPROCESSOR_PATH = os.path.join(OUTPUT_DIR, "preprocessor.joblib")
FEATURES_PATH = os.path.join(OUTPUT_DIR, "feature_names.joblib")  # T√™n features SAU KHI preprocessor transform
MODEL_PATHS = OrderedDict([
    ('Linear Regression', os.path.join(OUTPUT_DIR, "linear_regression_model.joblib")),
    ('Decision Tree', os.path.join(OUTPUT_DIR, "decision_tree_model.joblib")),
    ('Random Forest', os.path.join(OUTPUT_DIR, "random_forest_model.joblib")),
    ('XGBoost', os.path.join(OUTPUT_DIR, "xgboost_model.joblib")),
    ('MLP Regressor', os.path.join(OUTPUT_DIR, "mlp_regressor_model.joblib"))
])

HOURS_PER_PERSON_MONTH = 152  # S·ªë gi·ªù l√†m vi·ªác trung b√¨nh m·ªói th√°ng cho m·ªôt ng∆∞·ªùi


# --- H√†m t·∫£i Artifacts (D·ª±a tr√™n app.py b·∫°n cung c·∫•p) ---
@st.cache_resource  # Cache ƒë·ªÉ kh√¥ng t·∫£i l·∫°i m·ªói l·∫ßn rerun
def load_all_artifacts_from_files(preprocessor_path, features_path, model_paths_dict):
    """
    T·∫£i preprocessor, feature names (sau x·ª≠ l√Ω), c√°c m√¥ h√¨nh ML,
    v√† tr√≠ch xu·∫•t th·ª© t·ª± c·ªôt g·ªëc c√πng c√°c t√πy ch·ªçn cho c·ªôt ph√¢n lo·∫°i t·ª´ preprocessor.
    Tr·∫£ v·ªÅ: preprocessor, feature_names_processed, loaded_models, original_cols_order, categorical_features_options, load_status_flag
    """
    loaded_models = OrderedDict()
    preprocessor = None
    feature_names_processed = None  # T√™n features SAU KHI preprocessor transform
    categorical_features_options = {}  # L∆∞u c√°c categories cho m·ªói c·ªôt categorical g·ªëc
    original_cols_order = []  # Th·ª© t·ª± c√°c c·ªôt g·ªëc m√† preprocessor mong ƒë·ª£i
    all_loaded_successfully = True  # C·ªù theo d√µi tr·∫°ng th√°i t·∫£i

    # 1. T·∫£i Preprocessor
    if not os.path.exists(preprocessor_path):
        st.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file preprocessor t·∫°i '{preprocessor_path}'.")
        return None, None, None, None, None, False  # Th√™m c·ªù tr·∫°ng th√°i t·∫£i
    try:
        preprocessor = joblib.load(preprocessor_path)
        st.sidebar.success(f"Preprocessor: T·∫£i th√†nh c√¥ng.")

        # Tr√≠ch xu·∫•t th√¥ng tin t·ª´ Preprocessor
        try:
            # T√¨m transformer cho numerical v√† categorical features
            # T√™n 'num' v√† 'cat' ph·∫£i kh·ªõp v·ªõi t√™n b·∫°n ƒë·∫∑t trong ColumnTransformer
            num_transformer_info = next(t for t in preprocessor.transformers_ if t[0] == 'num')
            cat_transformer_info = next(t for t in preprocessor.transformers_ if t[0] == 'cat')

            original_num_features = list(num_transformer_info[2])  # Danh s√°ch t√™n c·ªôt s·ªë g·ªëc
            original_cat_features = list(cat_transformer_info[2])  # Danh s√°ch t√™n c·ªôt ph√¢n lo·∫°i g·ªëc
            original_cols_order = original_num_features + original_cat_features

            # Tr√≠ch xu·∫•t categories t·ª´ OneHotEncoder b√™n trong pipeline c·ªßa 'cat'
            cat_pipeline = preprocessor.named_transformers_['cat']  # Ho·∫∑c t√™n pipeline c·ªßa b·∫°n
            onehot_encoder = cat_pipeline.named_steps['onehot']  # Ho·∫∑c t√™n b∆∞·ªõc onehot c·ªßa b·∫°n

            if hasattr(onehot_encoder, 'categories_'):
                if len(onehot_encoder.categories_) == len(original_cat_features):
                    for i, feature_name in enumerate(original_cat_features):
                        categories = onehot_encoder.categories_[i]
                        # Lo·∫°i b·ªè np.nan n·∫øu c√≥ trong categories (th∆∞·ªùng do SimpleImputer t·∫°o ra)
                        # v√† chuy·ªÉn th√†nh string ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n cho selectbox
                        cleaned_categories = [str(cat) for cat in categories if
                                              not (isinstance(cat, float) and np.isnan(cat))]
                        categorical_features_options[feature_name] = cleaned_categories
                    st.sidebar.caption("ƒê√£ tr√≠ch xu·∫•t t√πy ch·ªçn cho c·ªôt ph√¢n lo·∫°i.")
                else:
                    st.error(
                        f"L·ªói tr√≠ch xu·∫•t preprocessor: S·ªë l∆∞·ª£ng categories ({len(onehot_encoder.categories_)}) kh√¥ng kh·ªõp s·ªë c·ªôt ph√¢n lo·∫°i ({len(original_cat_features)}).")
                    all_loaded_successfully = False
            else:
                st.error("L·ªói tr√≠ch xu·∫•t preprocessor: Kh√¥ng t√¨m th·∫•y 'categories_' trong OneHotEncoder.")
                all_loaded_successfully = False
        except StopIteration:
            st.error(
                "L·ªói tr√≠ch xu·∫•t preprocessor: Kh√¥ng t√¨m th·∫•y transformer 'num' ho·∫∑c 'cat'. Ki·ªÉm tra t√™n trong ColumnTransformer.")
            all_loaded_successfully = False
        except KeyError as ke:
            st.error(f"L·ªói tr√≠ch xu·∫•t preprocessor: Kh√¥ng t√¨m th·∫•y step '{ke}' trong pipeline. Ki·ªÉm tra t√™n step.")
            all_loaded_successfully = False
        except Exception as e_extract:
            st.error(f"L·ªói khi tr√≠ch xu·∫•t th√¥ng tin t·ª´ preprocessor: {e_extract}")
            all_loaded_successfully = False
            print(traceback.format_exc())
    except Exception as e_load_prep:
        st.error(f"L·ªói nghi√™m tr·ªçng khi t·∫£i preprocessor: {e_load_prep}")
        print(traceback.format_exc())
        return None, None, None, None, None, False

    # 2. T·∫£i Feature Names (sau khi x·ª≠ l√Ω b·ªüi preprocessor)
    if not os.path.exists(features_path):
        st.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file t√™n ƒë·∫∑c tr∆∞ng (ƒë√£ x·ª≠ l√Ω) t·∫°i '{features_path}'.")
        all_loaded_successfully = False
    else:
        try:
            feature_names_processed = joblib.load(features_path)
            if isinstance(feature_names_processed, np.ndarray):
                feature_names_processed = feature_names_processed.tolist()
            if not isinstance(feature_names_processed, list):
                st.warning(
                    f"ƒê·ªãnh d·∫°ng feature_names_processed kh√¥ng ph·∫£i list (ki·ªÉu: {type(feature_names_processed)}). C·ªë g·∫Øng chuy·ªÉn ƒë·ªïi.")
                try:
                    feature_names_processed = list(feature_names_processed)
                except TypeError:
                    st.error("Kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi feature_names_processed th√†nh list.")
                    all_loaded_successfully = False
            if feature_names_processed and all_loaded_successfully:  # Ch·ªâ th√¥ng b√°o n·∫øu ch∆∞a c√≥ l·ªói
                st.sidebar.success(f"T√™n ƒë·∫∑c tr∆∞ng (ƒë√£ x·ª≠ l√Ω): T·∫£i th√†nh c√¥ng.")
        except Exception as e_load_feat:
            st.error(f"L·ªói khi t·∫£i feature_names_processed: {e_load_feat}")
            all_loaded_successfully = False

    # 3. T·∫£i c√°c M√¥ h√¨nh ML
    models_actually_loaded_count = 0
    for name, path in model_paths_dict.items():
        if not os.path.exists(path):
            st.warning(f"C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh '{name}' t·∫°i '{path}'. B·ªè qua.")
            continue
        try:
            model = joblib.load(path)
            loaded_models[name] = model
            models_actually_loaded_count += 1
        except Exception as e_load_model:
            st.warning(f"L·ªói khi t·∫£i m√¥ h√¨nh {name} t·ª´ '{path}': {e_load_model}. B·ªè qua.")

    if models_actually_loaded_count > 0:
        st.sidebar.success(
            f"M√¥ h√¨nh ML: T·∫£i th√†nh c√¥ng {models_actually_loaded_count}/{len(model_paths_dict)} m√¥ h√¨nh.")
    else:
        st.error("L·ªñI: Kh√¥ng t·∫£i ƒë∆∞·ª£c b·∫•t k·ª≥ m√¥ h√¨nh Machine Learning n√†o.")
        # Kh√¥ng nh·∫•t thi·∫øt ph·∫£i ƒë·∫∑t all_loaded_successfully = False ·ªü ƒë√¢y n·∫øu v·∫´n mu·ªën d√πng m√¥ h√¨nh truy·ªÅn th·ªëng

    # Ki·ªÉm tra cu·ªëi c√πng tr∆∞·ªõc khi tr·∫£ v·ªÅ
    if not preprocessor or not feature_names_processed or not original_cols_order:
        st.error("Thi·∫øu m·ªôt ho·∫∑c nhi·ªÅu th√†nh ph·∫ßn ML quan tr·ªçng (preprocessor, feature_names, original_cols_order).")
        all_loaded_successfully = False
        # Kh√¥ng tr·∫£ v·ªÅ categorical_features_options n·∫øu original_cols_order r·ªóng ho·∫∑c preprocessor l·ªói
        if not original_cols_order or not preprocessor: categorical_features_options = {}

    return preprocessor, feature_names_processed, loaded_models, original_cols_order, categorical_features_options, all_loaded_successfully


# --- Th·ª±c hi·ªán t·∫£i artifacts ---
preprocessor, feature_names_processed_from_file, ml_models_loaded, \
    original_cols_input_order, categorical_options_from_preprocessor, artifacts_load_status = load_all_artifacts_from_files(
    PREPROCESSOR_PATH, FEATURES_PATH, MODEL_PATHS
)


# --- H√†m t√≠nh to√°n cho m√¥ h√¨nh truy·ªÅn th·ªëng (T·ª´ app.py, tr·∫£ v·ªÅ Person-Hours) ---
def calculate_cocomo_basic(loc, mode, eaf, hrs_per_month):
    if loc <= 0: return "L·ªói (LOC <= 0)"
    if hrs_per_month <= 0: return "L·ªói (Gi·ªù/Th√°ng <= 0)"
    kloc = loc / 1000.0
    params = {"Organic": {"a": 2.4, "b": 1.05}, "Semi-detached": {"a": 3.0, "b": 1.12},
              "Embedded": {"a": 3.6, "b": 1.20}}
    if mode not in params: return "L·ªói (Ch·∫ø ƒë·ªô kh√¥ng h·ª£p l·ªá)"
    a, b = params[mode]["a"], params[mode]["b"]
    try:
        person_months = a * (kloc ** b) * eaf
        return max(0.0, round(person_months * hrs_per_month, 2))
    except Exception as e:
        return f"L·ªói t√≠nh to√°n COCOMO: {e}"


def calculate_fp_effort(fp, hrs_per_fp):
    if fp <= 0: return "L·ªói (FP <= 0)"
    if hrs_per_fp <= 0: return "L·ªói (Gi·ªù/FP <= 0)"
    try:
        return max(0.0, round(fp * hrs_per_fp, 2))
    except Exception as e:
        return f"L·ªói t√≠nh to√°n FP: {e}"


def calculate_ucp_effort(ucp, hrs_per_ucp):
    if ucp <= 0: return "L·ªói (UCP <= 0)"
    if hrs_per_ucp <= 0: return "L·ªói (Gi·ªù/UCP <= 0)"
    try:
        return max(0.0, round(ucp * hrs_per_ucp, 2))
    except Exception as e:
        return f"L·ªói t√≠nh to√°n UCP: {e}"


# --- Giao di·ªán Nh·∫≠p li·ªáu Sidebar ---
st.sidebar.header("Nh·∫≠p Th√¥ng tin D·ª± √°n")
input_values_for_ml = {}  # Dictionary cho c√°c gi√° tr·ªã s·∫Ω ƒë∆∞·ª£c truy·ªÅn v√†o preprocessor

# C√°c tr∆∞·ªùng nh·∫≠p li·ªáu c∆° b·∫£n (LOC, FP, UCP) - D√πng cho c·∫£ ML (n·∫øu c√≥ trong original_cols_input_order) v√† truy·ªÅn th·ªëng
st.sidebar.subheader("K√≠ch th∆∞·ªõc D·ª± √°n (∆Ø·ªõc t√≠nh)")
col_loc_fp, col_ucp_empty = st.sidebar.columns(2)
with col_loc_fp:
    # LOC (Lu√¥n hi·ªÉn th·ªã v√¨ c·∫ßn cho COCOMO)
    loc_input_val = st.number_input("Lines of Code (LOC)", min_value=0, value=10000, step=100, key="loc_input_v_adv")
    if original_cols_input_order and 'LOC' in original_cols_input_order:
        input_values_for_ml['LOC'] = loc_input_val

    # FP (Lu√¥n hi·ªÉn th·ªã v√¨ c·∫ßn cho t√≠nh to√°n FP)
    fp_input_val = st.number_input("Function Points (FP)", min_value=0, value=100, step=10, key="fp_input_v_adv")
    if original_cols_input_order and 'FP' in original_cols_input_order:
        input_values_for_ml['FP'] = fp_input_val
with col_ucp_empty:
    # UCP (Lu√¥n hi·ªÉn th·ªã v√¨ c·∫ßn cho t√≠nh to√°n UCP)
    ucp_input_val = st.number_input("Use Case Points (UCP)", min_value=0.0, value=100.0, step=10.0, format="%.2f",
                                    key="ucp_input_v_adv")
    if original_cols_input_order and 'UCP' in original_cols_input_order:
        input_values_for_ml['UCP'] = ucp_input_val

# C√°c tr∆∞·ªùng nh·∫≠p li·ªáu cho ML (t·∫°o ƒë·ªông d·ª±a tr√™n original_cols_input_order v√† categorical_options_from_preprocessor)
if artifacts_load_status and preprocessor and original_cols_input_order:
    st.sidebar.subheader("ƒê·∫∑c tr∆∞ng D·ª± √°n (Cho Model ML)")

    # L·∫•y danh s√°ch c·ªôt s·ªë v√† ph√¢n lo·∫°i g·ªëc t·ª´ original_cols_input_order v√† categorical_options_from_preprocessor
    original_numerical_cols_for_ui = [col for col in original_cols_input_order if
                                      col not in categorical_options_from_preprocessor and col not in ['LOC', 'FP',
                                                                                                       'UCP']]
    original_categorical_cols_for_ui = [col for col in original_cols_input_order if
                                        col in categorical_options_from_preprocessor]

    # Input cho c√°c c·ªôt s·ªë (tr·ª´ LOC, FP, UCP ƒë√£ c√≥ ·ªü tr√™n)
    if original_numerical_cols_for_ui:
        st.sidebar.markdown("**ƒê·∫∑c tr∆∞ng d·∫°ng s·ªë:**")
        num_cols_display = min(2, len(original_numerical_cols_for_ui)) if original_numerical_cols_for_ui else 1
        cols_num_ui = st.sidebar.columns(num_cols_display)

        for i, col_name in enumerate(original_numerical_cols_for_ui):
            current_col_container = cols_num_ui[i % num_cols_display]
            with current_col_container:
                default_val = 10;
                step_val = 1;
                min_val = 0
                if "month" in col_name.lower() or "time" in col_name.lower():
                    default_val = 6; min_val = 1
                elif "size" in col_name.lower():
                    default_val = 5; min_val = 1
                input_values_for_ml[col_name] = st.number_input(f"{col_name}", min_value=min_val, value=default_val,
                                                                step=step_val, key=f"ml_num_{col_name}")

    # Input cho c√°c c·ªôt ph√¢n lo·∫°i
    if original_categorical_cols_for_ui:
        st.sidebar.markdown("**ƒê·∫∑c tr∆∞ng d·∫°ng ph√¢n lo·∫°i:**")
        num_cat_cols_display = min(2, len(original_categorical_cols_for_ui)) if original_categorical_cols_for_ui else 1
        cols_cat_ui = st.sidebar.columns(num_cat_cols_display)

        for i, col_name in enumerate(original_categorical_cols_for_ui):
            current_col_cat_container = cols_cat_ui[i % num_cat_cols_display]
            with current_col_cat_container:
                options = categorical_options_from_preprocessor.get(col_name, [])
                if options:
                    default_index = 0
                    input_values_for_ml[col_name] = st.selectbox(f"{col_name}", options=options, index=default_index,
                                                                 key=f"ml_cat_{col_name}")
                else:  # Fallback n·∫øu kh√¥ng c√≥ options (d√π kh√¥ng n√™n x·∫£y ra n·∫øu preprocessor tr√≠ch xu·∫•t ƒë√∫ng)
                    st.sidebar.warning(
                        f"Kh√¥ng c√≥ t√πy ch·ªçn cho '{col_name}'. Nh·∫≠p th·ªß c√¥ng (n·∫øu preprocessor c·ªßa b·∫°n c√≥ th·ªÉ x·ª≠ l√Ω gi√° tr·ªã m·ªõi ho·∫∑c b·∫°n c√≥ imputer).")
                    input_values_for_ml[col_name] = st.text_input(f"{col_name} (nh·∫≠p tay)",
                                                                  key=f"ml_cat_text_{col_name}")
else:
    st.sidebar.warning(
        "Kh√¥ng th·ªÉ t·∫£i ƒë·∫ßy ƒë·ªß preprocessor ho·∫∑c th√¥ng tin c·ªôt. Ph·∫ßn nh·∫≠p li·ªáu chi ti·∫øt cho ML b·ªã h·∫°n ch·∫ø/v√¥ hi·ªáu h√≥a.")
    st.sidebar.info("Vui l√≤ng ki·ªÉm tra c√°c file: preprocessor.joblib, feature_names.joblib trong th∆∞ m·ª•c OUTPUT_DIR.")

# --- Widget nh·∫≠p li·ªáu cho M√¥ h√¨nh Truy·ªÅn th·ªëng ---
st.sidebar.subheader("Tham s·ªë cho M√¥ h√¨nh Truy·ªÅn th·ªëng")
# COCOMO II Basic
st.sidebar.markdown("**COCOMO II (Basic)**")
cocomo_mode_input = st.sidebar.selectbox("Ch·∫ø ƒë·ªô D·ª± √°n COCOMO", ["Organic", "Semi-detached", "Embedded"],
                                         key="cocomo_mode_input")
eaf_input = st.sidebar.number_input("H·ªá s·ªë ƒêi·ªÅu ch·ªânh N·ªó l·ª±c (EAF)", min_value=0.1, value=1.0, step=0.1, format="%.2f",
                                    key="eaf_input", help="Effort Adjustment Factor. 1.0 l√† nominal.")

# Function Points
st.sidebar.markdown("**Function Points (FP)**")
hours_per_fp_input = st.sidebar.number_input("NƒÉng su·∫•t (gi·ªù/FP)", min_value=0.1, value=10.0, step=0.5, format="%.1f",
                                             key="hrs_fp_input")

# Use Case Points
st.sidebar.markdown("**Use Case Points (UCP)**")
hours_per_ucp_input = st.sidebar.number_input("NƒÉng su·∫•t (gi·ªù/UCP)", min_value=0.1, value=20.0, step=1.0, format="%.1f",
                                              key="hrs_ucp_input")

# --- N√∫t D·ª± ƒëo√°n/T√≠nh to√°n ---
calculate_button = st.sidebar.button("üìä ∆Ø·ªõc t√≠nh & So s√°nh Effort", use_container_width=True, type="primary")

# --- Kh·ªüi t·∫°o session state cho k·∫øt qu·∫£ (n·∫øu ch∆∞a c√≥) ---
if 'raw_input_df_display' not in st.session_state:
    st.session_state.raw_input_df_display = None
if 'processed_input_df_display' not in st.session_state:
    st.session_state.processed_input_df_display = None

# --- X·ª≠ l√Ω v√† Hi·ªÉn th·ªã K·∫øt qu·∫£ ---
if calculate_button:
    st.divider()
    st.subheader("üìä K·∫øt qu·∫£ ∆Ø·ªõc t√≠nh Effort T·ªïng h·ª£p (Person-Hours)")

    all_estimation_results = OrderedDict()
    ml_error_messages = {}

    # --- 1. D·ª± ƒëo√°n t·ª´ M√¥ h√¨nh Machine Learning ---
    if artifacts_load_status and preprocessor and feature_names_processed_from_file and ml_models_loaded and original_cols_input_order:
        st.markdown("#### 1. D·ª± ƒëo√°n t·ª´ M√¥ h√¨nh Machine Learning")
        try:
            current_input_for_ml_df_values = {}
            missing_ml_inputs_runtime = []
            for col_orig_rt in original_cols_input_order:
                val = input_values_for_ml.get(col_orig_rt)
                current_input_for_ml_df_values[col_orig_rt] = val
                if val is None or (isinstance(val, str) and not val.strip()):  # Ki·ªÉm tra None ho·∫∑c chu·ªói r·ªóng
                    is_categorical_rt = col_orig_rt in categorical_options_from_preprocessor
                    if not is_categorical_rt:  # C·ªôt s·ªë
                        current_input_for_ml_df_values[col_orig_rt] = np.nan
                        # V·ªõi c·ªôt ph√¢n lo·∫°i, preprocessor (v·ªõi SimpleImputer(strategy='constant', fill_value='missing')
                    # v√† OneHotEncoder(handle_unknown='ignore')) s·∫Ω x·ª≠ l√Ω.
                    # N·∫øu kh√¥ng c√≥ imputer cho categorical, gi√° tr·ªã None/r·ªóng c√≥ th·ªÉ g√¢y l·ªói ·ªü OHE tr·ª´ khi n√≥ x·ª≠ l√Ω ƒë∆∞·ª£c.
                    missing_ml_inputs_runtime.append(f"{col_orig_rt} ({'ph√¢n lo·∫°i' if is_categorical_rt else 's·ªë'})")

            if missing_ml_inputs_runtime:
                st.caption(
                    f"ML Input: Gi√° tr·ªã None/r·ªóng/thi·∫øu cho: {', '.join(missing_ml_inputs_runtime)}. Preprocessor s·∫Ω c·ªë g·∫Øng x·ª≠ l√Ω.")

            input_df_for_preprocessor = pd.DataFrame([current_input_for_ml_df_values],
                                                     columns=original_cols_input_order)
            st.session_state.raw_input_df_display = input_df_for_preprocessor.copy()

            input_processed_np_array = preprocessor.transform(input_df_for_preprocessor)

            if isinstance(feature_names_processed_from_file, list) and len(feature_names_processed_from_file) == \
                    input_processed_np_array.shape[1]:
                input_processed_final_df = pd.DataFrame(input_processed_np_array,
                                                        columns=feature_names_processed_from_file)
                st.session_state.processed_input_df_display = input_processed_final_df.copy()

                for model_name, loaded_model_object in ml_models_loaded.items():
                    try:
                        prediction_raw = loaded_model_object.predict(input_processed_final_df)
                        # Gi·∫£ s·ª≠ m√¥ h√¨nh ML d·ª± ƒëo√°n Effort theo Person-Hours tr·ª±c ti·∫øp
                        prediction_value_ph = float(prediction_raw[0]) if prediction_raw.size > 0 else 0.0
                        all_estimation_results[f"ML: {model_name}"] = max(0.0, round(prediction_value_ph, 2))
                    except Exception as model_pred_e:
                        error_msg_detail = f"L·ªói d·ª± ƒëo√°n ({model_name}): {str(model_pred_e)}"
                        st.error(error_msg_detail)
                        all_estimation_results[f"ML: {model_name}"] = "L·ªói"
                        ml_error_messages[model_name] = str(model_pred_e)
            else:
                st.error(
                    f"L·ªói ML: S·ªë t√™n ƒë·∫∑c tr∆∞ng ƒë√£ x·ª≠ l√Ω ({len(feature_names_processed_from_file or [])}) kh√¥ng kh·ªõp s·ªë c·ªôt sau transform ({input_processed_np_array.shape[1]}).")
                for model_name_key_err in (ml_models_loaded.keys() if ml_models_loaded else MODEL_PATHS.keys()):
                    all_estimation_results[f"ML: {model_name_key_err}"] = "L·ªói (C·∫•u h√¨nh Feature)"
        except Exception as e_ml_main_process:
            st.error(f"L·ªói nghi√™m tr·ªçng trong ti·ªÅn x·ª≠ l√Ω/d·ª± ƒëo√°n ML: {e_ml_main_process}")
            for model_name_key_err_main in (ml_models_loaded.keys() if ml_models_loaded else MODEL_PATHS.keys()):
                all_estimation_results[f"ML: {model_name_key_err_main}"] = "L·ªói (Ti·ªÅn x·ª≠ l√Ω)"
            print(traceback.format_exc())
    else:
        st.info("D·ª± ƒëo√°n ML kh√¥ng th·ª±c hi·ªán do thi·∫øu th√†nh ph·∫ßn ho·∫∑c l·ªói t·∫£i artifacts.")

    # --- 2. T√≠nh to√°n t·ª´ M√¥ h√¨nh Truy·ªÅn th·ªëng (ƒê√£ tr·∫£ v·ªÅ Person-Hours) ---
    st.markdown("#### 2. T√≠nh to√°n t·ª´ M√¥ h√¨nh Truy·ªÅn th·ªëng")
    traditional_params_captions = []

    cocomo_effort_ph = calculate_cocomo_basic(loc_input_val, cocomo_mode_input, eaf_input, HOURS_PER_PERSON_MONTH)
    all_estimation_results['COCOMO II (Basic)'] = cocomo_effort_ph
    traditional_params_captions.append(
        f"* **COCOMO II (Basic):** Mode={cocomo_mode_input}, LOC={loc_input_val}, EAF={eaf_input}")

    fp_effort_ph = calculate_fp_effort(fp_input_val, hours_per_fp_input)
    all_estimation_results['Function Points'] = fp_effort_ph
    traditional_params_captions.append(f"* **Function Points:** FP={fp_input_val}, Hours/FP={hours_per_fp_input}")

    ucp_effort_ph = calculate_ucp_effort(ucp_input_val, hours_per_ucp_input)
    all_estimation_results['Use Case Points'] = ucp_effort_ph
    traditional_params_captions.append(f"* **Use Case Points:** UCP={ucp_input_val}, Hours/UCP={hours_per_ucp_input}")

    if traditional_params_captions:
        st.markdown("**Tham s·ªë s·ª≠ d·ª•ng cho m√¥ h√¨nh truy·ªÅn th·ªëng:**")
        for caption_text in traditional_params_captions: st.markdown(caption_text)
    st.caption("L∆∞u √Ω: K·∫øt qu·∫£ 'L·ªói' cho m√¥ h√¨nh truy·ªÅn th·ªëng xu·∫•t hi·ªán n·∫øu ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá.")

    # --- 3. Hi·ªÉn th·ªã B·∫£ng v√† Bi·ªÉu ƒë·ªì So s√°nh T·ªïng h·ª£p ---
    st.markdown("#### 3. B·∫£ng v√† Bi·ªÉu ƒë·ªì So s√°nh (Person-Hours)")

    if all_estimation_results:
        result_df_list = [{'M√¥ H√¨nh ∆Ø·ªõc T√≠nh': name, 'Effort D·ª± ƒëo√°n (person-hours)': effort} for name, effort in
                          all_estimation_results.items()]
        result_summary_df = pd.DataFrame(result_df_list)


        def format_effort_for_display(x_val):
            if isinstance(x_val, (int, float)): return f"{x_val:,.2f}"
            return str(x_val)


        st.write("B·∫£ng so s√°nh k·∫øt qu·∫£:")
        st.dataframe(
            result_summary_df.style.format({'Effort D·ª± ƒëo√°n (person-hours)': format_effort_for_display}),
            use_container_width=True, hide_index=True
        )

        st.write("Bi·ªÉu ƒë·ªì so s√°nh:")
        try:
            chart_df_source = result_summary_df.copy()
            chart_df_source['Effort D·ª± ƒëo√°n (person-hours)'] = chart_df_source['Effort D·ª± ƒëo√°n (person-hours)'].astype(
                str).str.replace(',', '', regex=False)
            chart_df_source['Effort D·ª± ƒëo√°n (person-hours)'] = pd.to_numeric(
                chart_df_source['Effort D·ª± ƒëo√°n (person-hours)'], errors='coerce')
            chart_df_source.dropna(subset=['Effort D·ª± ƒëo√°n (person-hours)'], inplace=True)

            if not chart_df_source.empty:
                chart_df_source = chart_df_source.sort_values(by='Effort D·ª± ƒëo√°n (person-hours)', ascending=False)
                # S·ª≠ d·ª•ng st.bar_chart() c·ªßa Streamlit thay v√¨ Matplotlib tr·ª±c ti·∫øp cho ƒë∆°n gi·∫£n v√† t∆∞∆°ng th√≠ch t·ªët h∆°n
                chart_data_for_st = chart_df_source.set_index('M√¥ H√¨nh ∆Ø·ªõc T√≠nh')['Effort D·ª± ƒëo√°n (person-hours)']
                st.bar_chart(chart_data_for_st)
            else:
                st.info("Kh√¥ng c√≥ d·ª± ƒëo√°n/t√≠nh to√°n h·ª£p l·ªá (ki·ªÉu s·ªë) ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì so s√°nh.")
        except Exception as chart_render_e:
            st.warning(f"Kh√¥ng th·ªÉ v·∫Ω bi·ªÉu ƒë·ªì so s√°nh: {chart_render_e}")
            print(traceback.format_exc())
    else:
        st.warning("Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë·ªÉ hi·ªÉn th·ªã.")

    if ml_error_messages:
        st.subheader("‚ö†Ô∏è Chi ti·∫øt l·ªói d·ª± ƒëo√°n ML:")
        for model_name_err_disp, msg_err_disp in ml_error_messages.items():
            st.caption(f"**{model_name_err_disp}:** {msg_err_disp}")

    st.info("""
    **L∆∞u √Ω quan tr·ªçng:** K·∫øt qu·∫£ ch·ªâ l√† ∆∞·ªõc t√≠nh. Effort th·ª±c t·∫ø c√≥ th·ªÉ kh√°c bi·ªát.
    ƒê·ªô ch√≠nh x√°c c·ªßa ML ph·ª• thu·ªôc v√†o d·ªØ li·ªáu hu·∫•n luy·ªán v√† preprocessor.
    ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh truy·ªÅn th·ªëng ph·ª• thu·ªôc v√†o vi·ªác ch·ªçn ƒë√∫ng tham s·ªë.
    """)

# X·ª≠ l√Ω tr∆∞·ªùng h·ª£p kh√¥ng t·∫£i ƒë∆∞·ª£c artifacts ban ƒë·∫ßu (hi·ªÉn th·ªã th√¥ng b√°o n·∫øu n√∫t ch∆∞a ƒë∆∞·ª£c nh·∫•n V√Ä artifacts l·ªói)
elif not calculate_button and not artifacts_load_status:
    st.error("Kh√¥ng th·ªÉ t·∫£i c√°c th√†nh ph·∫ßn ML c·∫ßn thi·∫øt. Ph·∫ßn d·ª± ƒëo√°n ML s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")
    st.info(f"Ki·ªÉm tra c√°c file .joblib trong th∆∞ m·ª•c OUTPUT_DIR (hi·ªán t·∫°i: '{os.path.abspath(OUTPUT_DIR)}').")
    st.info("B·∫°n v·∫´n c√≥ th·ªÉ s·ª≠ d·ª•ng t√≠nh to√°n t·ª´ m√¥ h√¨nh truy·ªÅn th·ªëng.")

# --- Ch√¢n trang ---
st.markdown("---")
st.caption(f"·ª®ng d·ª•ng demo. C√°c file artifacts ƒë∆∞·ª£c t√¨m ki·∫øm trong: {os.path.abspath(OUTPUT_DIR)}")

