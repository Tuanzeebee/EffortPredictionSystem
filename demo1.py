# -*- coding: utf-8 -*-
"""
app.py: ·ª®ng d·ª•ng Web Streamlit ƒë·ªÉ d·ª± ƒëo√°n Effort
(Bao g·ªìm M√¥ h√¨nh ML, COCOMO II Basic, FP, UCP v√† So s√°nh).

(Phi√™n b·∫£n ƒë√£ s·ª≠a l·ªói "Unknown Categories", c·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n file,
v√† s·ª≠a l·ªói hi·ªÉn th·ªã bi·ªÉu ƒë·ªì)
"""

import streamlit as st
import pandas as pd
import numpy as np
import joblib
import os
from collections import OrderedDict
import math # C·∫ßn cho COCOMO
import traceback # Th√™m ƒë·ªÉ in l·ªói chi ti·∫øt

# Import c√°c l·ªõp c·∫ßn thi·∫øt (Gi·ªØ nguy√™n nh∆∞ c≈©)
try:
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline
    from sklearn.impute import SimpleImputer
    from sklearn.preprocessing import RobustScaler, OneHotEncoder # Ho·∫∑c StandardScaler
    from sklearn.linear_model import LinearRegression
    from sklearn.tree import DecisionTreeRegressor
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.neural_network import MLPRegressor
    import xgboost as xgb
except ImportError as e:
    st.error(f"L·ªói Import th∆∞ vi·ªán: {e}. H√£y ƒë·∫£m b·∫£o c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt (streamlit, pandas, scikit-learn, joblib, xgboost) ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t trong m√¥i tr∆∞·ªùng c·ªßa b·∫°n.")
    st.stop()


# --- C·∫•u h√¨nh Trang v√† T·∫£i Artifacts (Gi·ªØ nguy√™n nh∆∞ c≈©) ---

st.set_page_config(page_title="So s√°nh ∆Ø·ªõc t√≠nh Effort Ph·∫ßn m·ªÅm", layout="wide")

st.title("·ª®ng d·ª•ng So s√°nh ∆Ø·ªõc t√≠nh Effort Ph·∫ßn m·ªÅm üìä")
st.write("""
Nh·∫≠p th√¥ng tin d·ª± √°n ƒë·ªÉ nh·∫≠n ∆∞·ªõc t√≠nh effort (person-hours) t·ª´ nhi·ªÅu m√¥ h√¨nh Machine Learning
v√† c√°c ph∆∞∆°ng ph√°p truy·ªÅn th·ªëng (COCOMO II Basic, Function Points, Use Case Points).
""")

# ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n (Gi·ªØ nguy√™n)
OUTPUT_DIR = "."
PREPROCESSOR_PATH = os.path.join(OUTPUT_DIR, "preprocessor.joblib")
FEATURES_PATH = os.path.join(OUTPUT_DIR, "feature_names.joblib")
MODEL_PATHS = OrderedDict([
    ('Linear Regression', os.path.join(OUTPUT_DIR, "linear_regression_model.joblib")),
    ('Decision Tree', os.path.join(OUTPUT_DIR, "decision_tree_model.joblib")),
    ('Random Forest', os.path.join(OUTPUT_DIR, "random_forest_model.joblib")),
    ('XGBoost', os.path.join(OUTPUT_DIR, "xgboost_model.joblib")),
    ('MLP Regressor', os.path.join(OUTPUT_DIR, "mlp_regressor_model.joblib"))
])

@st.cache_resource
def load_all_artifacts(preprocessor_path, features_path, model_paths_dict):
    """
    T·∫£i preprocessor, feature names, c√°c m√¥ h√¨nh ML, v√† tr√≠ch xu·∫•t c√°c danh m·ª•c.
    (Gi·ªØ nguy√™n logic h√†m n√†y)
    """
    loaded_models = OrderedDict()
    preprocessor = None
    feature_names = None
    categorical_features_options = {}
    original_cols_order = []
    all_loaded_successfully = True

    # --- T·∫£i Preprocessor ---
    if not os.path.exists(preprocessor_path):
        st.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file preprocessor t·∫°i '{preprocessor_path}'")
        return None, None, None, None, None
    try:
        preprocessor = joblib.load(preprocessor_path)
        print("T·∫£i preprocessor th√†nh c√¥ng.")
        # --- Tr√≠ch xu·∫•t th√¥ng tin t·ª´ Preprocessor ---
        try:
            num_transformer_tuple = next(t for t in preprocessor.transformers_ if t[0] == 'num')
            cat_transformer_tuple = next(t for t in preprocessor.transformers_ if t[0] == 'cat')
            original_num_features = list(num_transformer_tuple[2])
            original_cat_features = list(cat_transformer_tuple[2])
            original_cols_order = original_num_features + original_cat_features
            print("Th·ª© t·ª± c·ªôt g·ªëc mong ƒë·ª£i:", original_cols_order)

            cat_pipeline = preprocessor.named_transformers_['cat']
            onehot_encoder = cat_pipeline.named_steps['onehot']

            if hasattr(onehot_encoder, 'categories_'):
                if len(onehot_encoder.categories_) == len(original_cat_features):
                    for i, feature_name in enumerate(original_cat_features):
                        categories = onehot_encoder.categories_[i]
                        categorical_features_options[feature_name] = categories.tolist()
                    print("Tr√≠ch xu·∫•t danh m·ª•c t·ª´ OneHotEncoder th√†nh c√¥ng.")
                else:
                    st.error(f"L·ªói: S·ªë l∆∞·ª£ng danh m·ª•c ({len(onehot_encoder.categories_)}) kh√¥ng kh·ªõp s·ªë c·ªôt ph√¢n lo·∫°i ({len(original_cat_features)}).")
                    all_loaded_successfully = False
            else:
                 st.error("L·ªói: Kh√¥ng t√¨m th·∫•y thu·ªôc t√≠nh 'categories_' trong OneHotEncoder.")
                 all_loaded_successfully = False
        except Exception as e_extract:
            st.error(f"L·ªói khi tr√≠ch xu·∫•t th√¥ng tin t·ª´ preprocessor: {e_extract}")
            all_loaded_successfully = False
    except Exception as e_load_prep:
        st.error(f"L·ªói nghi√™m tr·ªçng khi t·∫£i preprocessor: {e_load_prep}")
        return None, None, None, None, None

    # --- T·∫£i Feature Names (sau khi x·ª≠ l√Ω) ---
    if not os.path.exists(features_path):
        st.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file t√™n ƒë·∫∑c tr∆∞ng t·∫°i '{features_path}'")
        all_loaded_successfully = False
    else:
        try:
            feature_names = joblib.load(features_path)
            print("T·∫£i feature names (ƒë√£ x·ª≠ l√Ω) th√†nh c√¥ng.")
            if isinstance(feature_names, np.ndarray): feature_names = feature_names.tolist()
            if not isinstance(feature_names, list):
                 st.warning(f"ƒê·ªãnh d·∫°ng feature_names kh√¥ng ph·∫£i list (ki·ªÉu: {type(feature_names)}).")
                 try: feature_names = list(feature_names)
                 except TypeError:
                      st.error("Kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi feature_names th√†nh list.")
                      all_loaded_successfully = False
        except Exception as e_load_feat:
            st.error(f"L·ªói khi t·∫£i feature names: {e_load_feat}")
            all_loaded_successfully = False

    # --- T·∫£i c√°c M√¥ h√¨nh ML ---
    models_actually_loaded = 0
    for name, path in model_paths_dict.items():
        if not os.path.exists(path):
            st.warning(f"C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh '{name}' t·∫°i '{path}'. B·ªè qua.")
            continue
        try:
            model = joblib.load(path)
            loaded_models[name] = model
            models_actually_loaded += 1
            print(f"T·∫£i m√¥ h√¨nh ML {name} th√†nh c√¥ng.")
        except Exception as e_load_model:
            st.warning(f"L·ªói khi t·∫£i m√¥ h√¨nh {name}: {e_load_model}. B·ªè qua.")

    if models_actually_loaded == 0:
         st.error("L·ªñI: Kh√¥ng t·∫£i ƒë∆∞·ª£c b·∫•t k·ª≥ m√¥ h√¨nh Machine Learning n√†o.")
         # Kh√¥ng nh·∫•t thi·∫øt ph·∫£i d·ª´ng ho√†n to√†n n·∫øu ng∆∞·ªùi d√πng v·∫´n mu·ªën d√πng m√¥ h√¨nh truy·ªÅn th·ªëng
         # all_loaded_successfully = False # T·∫°m th·ªùi comment n·∫øu mu·ªën ch·∫°y ti·∫øp ch·ªâ v·ªõi m√¥ h√¨nh truy·ªÅn th·ªëng

    if all_loaded_successfully and preprocessor and feature_names and original_cols_order and categorical_features_options:
        return preprocessor, feature_names, loaded_models, original_cols_order, categorical_features_options
    else:
        st.error("M·ªôt ho·∫∑c nhi·ªÅu th√†nh ph·∫ßn ML quan tr·ªçng kh√¥ng th·ªÉ t·∫£i ho·∫∑c x·ª≠ l√Ω.")
        # Tr·∫£ v·ªÅ nh·ªØng g√¨ ƒë√£ t·∫£i ƒë∆∞·ª£c ƒë·ªÉ c√≥ th·ªÉ v·∫´n d√πng ƒë∆∞·ª£c ph·∫ßn kh√°c n·∫øu mu·ªën
        return preprocessor, feature_names, loaded_models, original_cols_order, categorical_features_options

# --- Th·ª±c hi·ªán t·∫£i ---
preprocessor, feature_names_out, ml_models, original_cols_order, categorical_features_options = load_all_artifacts(
    PREPROCESSOR_PATH, FEATURES_PATH, MODEL_PATHS
)

# --- H√†m t√≠nh to√°n cho m√¥ h√¨nh truy·ªÅn th·ªëng ---

def calculate_cocomo_basic(loc, mode, eaf, hrs_per_month):
    """T√≠nh to√°n effort theo COCOMO II Basic (quy ƒë·ªïi ra person-hours)."""
    if loc <= 0:
        return "L·ªói (LOC <= 0)"
    if hrs_per_month <= 0:
        return "L·ªói (Gi·ªù/Th√°ng <= 0)"

    kloc = loc / 1000.0

    # Tham s·ªë COCOMO II Basic
    params = {
        "Organic": {"a": 2.4, "b": 1.05},
        "Semi-detached": {"a": 3.0, "b": 1.12},
        "Embedded": {"a": 3.6, "b": 1.20}
    }

    if mode not in params:
        return "L·ªói (Ch·∫ø ƒë·ªô kh√¥ng h·ª£p l·ªá)"

    a = params[mode]["a"]
    b = params[mode]["b"]

    try:
        # C√¥ng th·ª©c c∆° b·∫£n: Effort (Person-Months) = a * (KLOC)^b * EAF
        person_months = a * (kloc ** b) * eaf
        person_hours = person_months * hrs_per_month
        return max(0.0, round(person_hours, 2)) # ƒê·∫£m b·∫£o kh√¥ng √¢m
    except Exception as e:
        return f"L·ªói t√≠nh to√°n: {e}"

def calculate_fp_effort(fp, hrs_per_fp):
    """T√≠nh to√°n effort d·ª±a tr√™n Function Points."""
    if fp <= 0:
        return "L·ªói (FP <= 0)"
    if hrs_per_fp <= 0:
        return "L·ªói (Gi·ªù/FP <= 0)"
    try:
        person_hours = fp * hrs_per_fp
        return max(0.0, round(person_hours, 2))
    except Exception as e:
        return f"L·ªói t√≠nh to√°n: {e}"

def calculate_ucp_effort(ucp, hrs_per_ucp):
    """T√≠nh to√°n effort d·ª±a tr√™n Use Case Points."""
    if ucp <= 0:
        return "L·ªói (UCP <= 0)"
    if hrs_per_ucp <= 0:
        return "L·ªói (Gi·ªù/UCP <= 0)"
    try:
        person_hours = ucp * hrs_per_ucp
        return max(0.0, round(person_hours, 2))
    except Exception as e:
        return f"L·ªói t√≠nh to√°n: {e}"


# --- T·∫°o Giao di·ªán Nh·∫≠p li·ªáu ---
# (Code gi·ªØ nguy√™n nh∆∞ tr∆∞·ªõc)
st.sidebar.header("Nh·∫≠p Th√¥ng tin D·ª± √°n")
input_values = {} # Dictionary chung ƒë·ªÉ l∆∞u t·∫•t c·∫£ gi√° tr·ªã ng∆∞·ªùi d√πng nh·∫≠p

# --- Widget nh·∫≠p li·ªáu cho ML v√† M√¥ h√¨nh truy·ªÅn th·ªëng ---
# Nh√≥m c√°c input c·∫ßn cho c·∫£ ML v√† truy·ªÅn th·ªëng l·∫°i v·ªõi nhau
st.sidebar.subheader("ƒê·∫∑c tr∆∞ng C∆° b·∫£n (S·ª≠ d·ª•ng b·ªüi nhi·ªÅu m√¥ h√¨nh)")
col1, col2 = st.sidebar.columns(2)
with col1:
    # LOC c·∫ßn cho ML (n·∫øu c√≥) v√† COCOMO
    if 'LOC' in original_cols_order or True: # Lu√¥n hi·ªÉn th·ªã LOC v√¨ c·∫ßn cho COCOMO
        input_values['LOC'] = st.number_input("Lines of Code (LOC)", min_value=0, value=10000, step=100, key="loc_input")
    # FP c·∫ßn cho ML (n·∫øu c√≥) v√† FP Estimation
    if 'FP' in original_cols_order or True: # Lu√¥n hi·ªÉn th·ªã FP
        input_values['FP'] = st.number_input("Function Points (FP)", min_value=0, value=100, step=10, key="fp_input")
with col2:
     # UCP c·∫ßn cho ML (n·∫øu c√≥) v√† UCP Estimation
    if 'UCP' in original_cols_order or True: # Lu√¥n hi·ªÉn th·ªã UCP
        input_values['UCP'] = st.number_input("Use Case Points (UCP)", min_value=0.0, value=100.0, step=10.0, format="%.2f", key="ucp_input")

# --- Widget nh·∫≠p li·ªáu ch·ªâ cho ML ---
# Ch·ªâ hi·ªÉn th·ªã n·∫øu preprocessor v√† c√°c th√†nh ph·∫ßn li√™n quan ƒë√£ ƒë∆∞·ª£c t·∫£i
if preprocessor and original_cols_order and categorical_features_options:
    st.sidebar.subheader("ƒê·∫∑c tr∆∞ng B·ªï sung (Ch·ªß y·∫øu cho ML)")
    col_ml1, col_ml2 = st.sidebar.columns(2)
    with col_ml1:
        if 'Development Time (months)' in original_cols_order:
            input_values['Development Time (months)'] = st.number_input("Development Time (months)", min_value=1, value=6, step=1)
        # Th√™m c√°c input s·ªë kh√°c cho ML n·∫øu c√≥
    with col_ml2:
        if 'Team Size' in original_cols_order:
            input_values['Team Size'] = st.number_input("Team Size", min_value=1, value=5, step=1)
        # Th√™m c√°c input s·ªë kh√°c cho ML n·∫øu c√≥

    st.sidebar.subheader("Th√¥ng tin Ph√¢n lo·∫°i (Ch·ªß y·∫øu cho ML)")
    col_cat1, col_cat2 = st.sidebar.columns(2)
    categorical_cols_with_options = list(categorical_features_options.keys())
    with col_cat1:
        for i, col_name in enumerate(categorical_cols_with_options):
            if col_name in original_cols_order and col_name in categorical_features_options:
                if i < len(categorical_cols_with_options) / 2:
                    options = categorical_features_options[col_name]
                    input_values[col_name] = st.selectbox(f"{col_name}", options=options, index=0, key=f"sb_{col_name}_1")
    with col_cat2:
        for i, col_name in enumerate(categorical_cols_with_options):
            if col_name in original_cols_order and col_name in categorical_features_options:
                if i >= len(categorical_cols_with_options) / 2:
                    options = categorical_features_options[col_name]
                    input_values[col_name] = st.selectbox(f"{col_name}", options=options, index=0, key=f"sb_{col_name}_2")
else:
    st.sidebar.warning("Kh√¥ng th·ªÉ t·∫£i preprocessor ho·∫∑c th√¥ng tin c·ªôt ML. Ph·∫ßn nh·∫≠p li·ªáu cho ML b·ªã v√¥ hi·ªáu h√≥a.")

# --- Widget nh·∫≠p li·ªáu cho M√¥ h√¨nh Truy·ªÅn th·ªëng ---
st.sidebar.subheader("Tham s·ªë cho M√¥ h√¨nh Truy·ªÅn th·ªëng")

# COCOMO II Basic
st.sidebar.markdown("**COCOMO II (Basic)**")
cocomo_mode = st.sidebar.selectbox("Ch·∫ø ƒë·ªô D·ª± √°n", ["Organic", "Semi-detached", "Embedded"], key="cocomo_mode")
eaf = st.sidebar.number_input("H·ªá s·ªë ƒêi·ªÅu ch·ªânh N·ªó l·ª±c (EAF)", min_value=0.1, value=1.0, step=0.1, format="%.2f", key="eaf", help="Effort Adjustment Factor - T√≠ch c√°c Cost Drivers. 1.0 l√† gi√° tr·ªã nominal.")
hours_per_month = st.sidebar.number_input("S·ªë gi·ªù l√†m vi·ªác/th√°ng (ƒë·ªÉ quy ƒë·ªïi)", min_value=1, value=152, step=8, key="hrs_month", help="D√πng ƒë·ªÉ chuy·ªÉn ƒë·ªïi Person-Months t·ª´ COCOMO sang Person-Hours.")

# Function Points
st.sidebar.markdown("**Function Points (FP)**")
hours_per_fp = st.sidebar.number_input("S·ªë gi·ªù/Function Point (NƒÉng su·∫•t)", min_value=0.1, value=10.0, step=0.5, format="%.1f", key="hrs_fp", help="NƒÉng su·∫•t ∆∞·ªõc t√≠nh c·ªßa nh√≥m ph√°t tri·ªÉn (v√≠ d·ª•: 5-20 gi·ªù/FP t√πy c√¥ng ngh·ªá, kinh nghi·ªám).")

# Use Case Points
st.sidebar.markdown("**Use Case Points (UCP)**")
hours_per_ucp = st.sidebar.number_input("S·ªë gi·ªù/Use Case Point (NƒÉng su·∫•t)", min_value=0.1, value=20.0, step=1.0, format="%.1f", key="hrs_ucp", help="H·ªá s·ªë nƒÉng su·∫•t cho UCP (th∆∞·ªùng trong kho·∫£ng 15-30 gi·ªù/UCP).")


# --- N√∫t D·ª± ƒëo√°n/T√≠nh to√°n ---
calculate_button = st.sidebar.button("üìä ∆Ø·ªõc t√≠nh & So s√°nh Effort", use_container_width=True, type="primary")


# --- X·ª≠ l√Ω v√† Hi·ªÉn th·ªã K·∫øt qu·∫£ ---
if calculate_button:
    st.divider()
    st.subheader("üìä K·∫øt qu·∫£ ∆Ø·ªõc t√≠nh Effort T·ªïng h·ª£p")

    all_results = OrderedDict() # L∆∞u t·∫•t c·∫£ k·∫øt qu·∫£ (ML v√† truy·ªÅn th·ªëng)
    error_messages = {}       # L∆∞u l·ªói c·ªßa ML

    # --- 1. D·ª± ƒëo√°n t·ª´ M√¥ h√¨nh Machine Learning ---
    # (Code gi·ªØ nguy√™n nh∆∞ tr∆∞·ªõc)
    if preprocessor and feature_names_out and ml_models and original_cols_order and categorical_features_options:
        st.markdown("#### 1. D·ª± ƒëo√°n t·ª´ M√¥ h√¨nh Machine Learning")
        # T·∫°o DataFrame cho ML input
        try:
            ordered_input_data_ml = {}
            missing_inputs_ml = []
            for col in original_cols_order: # Ch·ªâ l·∫•y c√°c c·ªôt m√† preprocessor c·∫ßn
                 if col in input_values:
                      ordered_input_data_ml[col] = input_values[col]
                 else:
                      missing_inputs_ml.append(col)
                      ordered_input_data_ml[col] = np.nan # Gi√° tr·ªã thi·∫øu s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi imputer trong preprocessor

            if missing_inputs_ml:
                 st.warning(f"ML Input: Thi·∫øu gi√° tr·ªã cho: {', '.join(missing_inputs_ml)}. S·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi imputer.")

            input_df_ml = pd.DataFrame([ordered_input_data_ml], columns=original_cols_order)

            # √Åp d·ª•ng preprocessor
            input_processed_np = preprocessor.transform(input_df_ml)

            # Chuy·ªÉn th√†nh DataFrame ƒë√£ x·ª≠ l√Ω
            if isinstance(feature_names_out, list) and len(feature_names_out) == input_processed_np.shape[1]:
                 input_processed_df = pd.DataFrame(input_processed_np, columns=feature_names_out)

                 # Th·ª±c hi·ªán d·ª± ƒëo√°n v·ªõi c√°c m√¥ h√¨nh ML
                 for model_name, loaded_model in ml_models.items():
                     try:
                         pred = loaded_model.predict(input_processed_df)
                         prediction_value = float(pred[0]) if pred.size > 0 else 0.0
                         all_results[f"ML: {model_name}"] = max(0.0, round(prediction_value, 2)) # Th√™m prefix "ML:"
                     except Exception as model_pred_e:
                         error_msg = f"L·ªói khi d·ª± ƒëo√°n b·∫±ng {model_name}: {str(model_pred_e)}"
                         st.error(error_msg)
                         all_results[f"ML: {model_name}"] = "L·ªói"
                         error_messages[model_name] = str(model_pred_e)

            else:
                 st.error(f"L·ªói ML: S·ªë l∆∞·ª£ng t√™n ƒë·∫∑c tr∆∞ng ({len(feature_names_out)}) kh√¥ng kh·ªõp s·ªë c·ªôt sau transform ({input_processed_np.shape[1]}).")
                 for model_name in ml_models.keys(): # ƒê√°nh d·∫•u l·ªói cho t·∫•t c·∫£ ML models
                     all_results[f"ML: {model_name}"] = "L·ªói (Config)"

        except Exception as e_ml_process:
            st.error(f"L·ªói nghi√™m tr·ªçng trong qu√° tr√¨nh x·ª≠ l√Ω/d·ª± ƒëo√°n ML: {e_ml_process}")
            for model_name in ml_models.keys(): # ƒê√°nh d·∫•u l·ªói cho t·∫•t c·∫£ ML models
                 all_results[f"ML: {model_name}"] = "L·ªói (Process)"
            # import traceback # ƒê√£ import ·ªü ƒë·∫ßu
            print("--- TRACEBACK L·ªñI ML ---")
            print(traceback.format_exc())
            print("---------------------")
    else:
        st.info("Ph·∫ßn d·ª± ƒëo√°n Machine Learning kh√¥ng ƒë∆∞·ª£c th·ª±c hi·ªán do thi·∫øu th√†nh ph·∫ßn c·∫ßn thi·∫øt (preprocessor, models...).")


    # --- 2. T√≠nh to√°n t·ª´ M√¥ h√¨nh Truy·ªÅn th·ªëng ---
    # (Code gi·ªØ nguy√™n nh∆∞ tr∆∞·ªõc)
    st.markdown("#### 2. T√≠nh to√°n t·ª´ M√¥ h√¨nh Truy·ªÅn th·ªëng")
    traditional_captions = [] # L∆∞u ch√∫ th√≠ch cho t·ª´ng m√¥ h√¨nh

    # L·∫•y gi√° tr·ªã input c·∫ßn thi·∫øt
    loc_val = input_values.get('LOC', 0)
    fp_val = input_values.get('FP', 0)
    ucp_val = input_values.get('UCP', 0)

    # COCOMO II Basic
    cocomo_effort = calculate_cocomo_basic(loc_val, cocomo_mode, eaf, hours_per_month)
    all_results['COCOMO II (Basic)'] = cocomo_effort
    traditional_captions.append(f"* **COCOMO II (Basic):** Mode={cocomo_mode}, LOC={loc_val}, EAF={eaf}, Hours/Month={hours_per_month}")

    # Function Points
    fp_effort = calculate_fp_effort(fp_val, hours_per_fp)
    all_results['Function Points'] = fp_effort
    traditional_captions.append(f"* **Function Points:** FP={fp_val}, Hours/FP={hours_per_fp}")

    # Use Case Points
    ucp_effort = calculate_ucp_effort(ucp_val, hours_per_ucp)
    all_results['Use Case Points'] = ucp_effort
    traditional_captions.append(f"* **Use Case Points:** UCP={ucp_val}, Hours/UCP={hours_per_ucp}")

    st.markdown("**Tham s·ªë s·ª≠ d·ª•ng:**")
    for caption in traditional_captions:
        st.markdown(caption)
    st.caption("L∆∞u √Ω: K·∫øt qu·∫£ 'L·ªói' xu·∫•t hi·ªán n·∫øu ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá (v√≠ d·ª•: LOC, FP, UCP <= 0 ho·∫∑c nƒÉng su·∫•t <= 0).")


    # --- 3. Hi·ªÉn th·ªã B·∫£ng v√† Bi·ªÉu ƒë·ªì So s√°nh T·ªïng h·ª£p ---
    st.markdown("#### 3. B·∫£ng v√† Bi·ªÉu ƒë·ªì So s√°nh")

    if all_results:
        # Chuy·ªÉn dictionary th√†nh DataFrame
        result_list = []
        for model_name, effort in all_results.items():
             result_list.append({'M√¥ H√¨nh ∆Ø·ªõc T√≠nh': model_name, 'Effort D·ª± ƒëo√°n (person-hours)': effort})
        result_df = pd.DataFrame(result_list)

        # ƒê·ªãnh d·∫°ng c·ªôt s·ªë v√† x·ª≠ l√Ω gi√° tr·ªã "L·ªói" cho b·∫£ng
        def format_effort_display(x):
            if isinstance(x, (int, float)):
                return f"{x:,.2f}" # ƒê·ªãnh d·∫°ng s·ªë
            return str(x) # Gi·ªØ nguy√™n chu·ªói (v√≠ d·ª•: "L·ªói...")

        st.write("B·∫£ng so s√°nh k·∫øt qu·∫£:")
        st.dataframe(
             result_df.style.format({'Effort D·ª± ƒëo√°n (person-hours)': format_effort_display}),
             use_container_width=True,
             hide_index=True # ·∫®n index c·ªßa DataFrame
        )

        # *** S·ª¨A L·ªñI BI·ªÇU ƒê·ªí ***
        # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì c·ªôt so s√°nh
        st.write("Bi·ªÉu ƒë·ªì so s√°nh:")
        try:
             # T·∫°o b·∫£n sao ƒë·ªÉ x·ª≠ l√Ω cho bi·ªÉu ƒë·ªì
             chart_df = result_df.copy()

             # 1. Chuy·ªÉn ƒë·ªïi c·ªôt Effort sang s·ªë, l·ªói th√†nh NaN
             #    ƒê·∫£m b·∫£o c·ªôt l√† ki·ªÉu string tr∆∞·ªõc khi thay th·∫ø ƒë·ªÉ tr√°nh l·ªói Attribute
             chart_df['Effort D·ª± ƒëo√°n (person-hours)'] = chart_df['Effort D·ª± ƒëo√°n (person-hours)'].astype(str).str.replace(',', '', regex=False) # X√≥a d·∫•u ph·∫©y n·∫øu c√≥
             chart_df['Effort D·ª± ƒëo√°n (person-hours)'] = pd.to_numeric(chart_df['Effort D·ª± ƒëo√°n (person-hours)'], errors='coerce')

             # 2. L·ªçc b·ªè c√°c h√†ng c√≥ gi√° tr·ªã NaN (l·ªói ho·∫∑c kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi)
             chart_df.dropna(subset=['Effort D·ª± ƒëo√°n (person-hours)'], inplace=True)

             # 3. Ki·ªÉm tra n·∫øu c√≤n d·ªØ li·ªáu h·ª£p l·ªá
             if not chart_df.empty:
                  # ƒê·∫∑t t√™n m√¥ h√¨nh l√†m index cho bi·ªÉu ƒë·ªì
                  chart_data = chart_df.set_index('M√¥ H√¨nh ∆Ø·ªõc T√≠nh')['Effort D·ª± ƒëo√°n (person-hours)']
                  st.bar_chart(chart_data)
             else:
                  st.info("Kh√¥ng c√≥ d·ª± ƒëo√°n/t√≠nh to√°n h·ª£p l·ªá (ki·ªÉu s·ªë) ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì.")

        except Exception as chart_e:
             st.warning(f"Kh√¥ng th·ªÉ v·∫Ω bi·ªÉu ƒë·ªì so s√°nh: {chart_e}")
             print("--- TRACEBACK L·ªñI BI·ªÇU ƒê·ªí ---")
             print(traceback.format_exc())
             print("--------------------------")
        # *** K·∫æT TH√öC S·ª¨A L·ªñI BI·ªÇU ƒê·ªí ***

    else:
         st.warning("Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë·ªÉ hi·ªÉn th·ªã.")

    # Hi·ªÉn th·ªã chi ti·∫øt l·ªói ML n·∫øu c√≥
    if error_messages:
         st.subheader("‚ö†Ô∏è Chi ti·∫øt l·ªói d·ª± ƒëo√°n ML:")
         for model_name, msg in error_messages.items():
              st.caption(f"**{model_name}:** {msg}")

    st.info("""
    **L∆∞u √Ω quan tr·ªçng:**
    * K·∫øt qu·∫£ t·ª´ c√°c m√¥ h√¨nh (ML v√† truy·ªÅn th·ªëng) ch·ªâ l√† **∆∞·ªõc t√≠nh**. Effort th·ª±c t·∫ø c√≥ th·ªÉ kh√°c bi·ªát ƒë√°ng k·ªÉ do nhi·ªÅu y·∫øu t·ªë kh√¥ng ƒë∆∞·ª£c m√¥ h√¨nh h√≥a.
    * ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh ML ph·ª• thu·ªôc v√†o ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu hu·∫•n luy·ªán.
    * ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh truy·ªÅn th·ªëng ph·ª• thu·ªôc v√†o vi·ªác ch·ªçn ƒë√∫ng tham s·ªë (EAF, ch·∫ø ƒë·ªô COCOMO, nƒÉng su·∫•t FP/UCP) ph√π h·ª£p v·ªõi d·ª± √°n v√† m√¥i tr∆∞·ªùng c·ª• th·ªÉ.
    * H√£y s·ª≠ d·ª•ng c√°c k·∫øt qu·∫£ n√†y nh∆∞ m·ªôt ƒëi·ªÉm tham kh·∫£o v√† k·∫øt h·ª£p v·ªõi kinh nghi·ªám th·ª±c t·∫ø ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh cu·ªëi c√πng.
    """)


# --- X·ª≠ l√Ω tr∆∞·ªùng h·ª£p kh√¥ng t·∫£i ƒë∆∞·ª£c artifacts ban ƒë·∫ßu ---
# (Gi·ªØ nguy√™n ph·∫ßn n√†y)
elif not ml_models and not preprocessor: # N·∫øu c·∫£ ML models v√† preprocessor ƒë·ªÅu l·ªói
     st.error("Kh√¥ng th·ªÉ t·∫£i c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt cho d·ª± ƒëo√°n Machine Learning (preprocessor, models).")
     st.info("B·∫°n v·∫´n c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c t√≠nh to√°n t·ª´ m√¥ h√¨nh truy·ªÅn th·ªëng n·∫øu nh·∫≠p ƒë·ªß th√¥ng tin.")
elif not ml_models:
     st.warning("Kh√¥ng t·∫£i ƒë∆∞·ª£c b·∫•t k·ª≥ m√¥ h√¨nh Machine Learning n√†o. Ch·ªâ c√≥ th·ªÉ s·ª≠ d·ª•ng t√≠nh to√°n t·ª´ m√¥ h√¨nh truy·ªÅn th·ªëng.")
     st.info(f"Ki·ªÉm tra c√°c file .joblib trong th∆∞ m·ª•c '{OUTPUT_DIR}'.")
elif not preprocessor or not feature_names_out or not original_cols_order or not categorical_features_options:
     st.error("Kh√¥ng th·ªÉ t·∫£i ho·∫∑c x·ª≠ l√Ω preprocessor ho·∫∑c th√¥ng tin ƒë·∫∑c tr∆∞ng cho ML.")
     st.info("Ph·∫ßn d·ª± ƒëo√°n Machine Learning s·∫Ω kh√¥ng ho·∫°t ƒë·ªông. B·∫°n v·∫´n c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c t√≠nh to√°n t·ª´ m√¥ h√¨nh truy·ªÅn th·ªëng.")
     st.info(f"Ki·ªÉm tra c√°c file '{PREPROCESSOR_PATH}' v√† '{FEATURES_PATH}'.")

# --- Ch√¢n trang ---
st.markdown("---")
st.caption("·ª®ng d·ª•ng demo ƒë∆∞·ª£c x√¢y d·ª±ng v·ªõi Streamlit, Scikit-learn, XGBoost v√† c√°c m√¥ h√¨nh ∆∞·ªõc t√≠nh truy·ªÅn th·ªëng.")
